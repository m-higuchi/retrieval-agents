# Retrieval Agents

[![CI](https://github.com/m-higuchi/retrieval-agents/actions/workflows/unit-tests.yml/badge.svg?branch=main)](https://github.com/m-higuchi/retrieval-agents/actions/workflows/unit-tests.yml)

## Ê¶ÇË¶Å

„Åì„ÅÆ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅØ„ÄÅRetrieval-Augmented Generation (RAG) „ÇíÊ¥ªÁî®„Åó„ÅüÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´ (LLM) „ÅÆ„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥ÈñãÁô∫„ÇíÁõÆÊåá„Åô„ÄÇRAG„ÅØ„ÄÅÂ§ñÈÉ®„Éá„Éº„Çø„ÇΩ„Éº„Çπ„ÇíÂà©Áî®„Åó„Å¶ÁîüÊàê„Åô„Çã„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÅÆÁ≤æÂ∫¶„ÇíÈ´ò„ÇÅ„ÇãÊäÄË°ì„Åß„ÅÇ„Çä„ÄÅÊú¨„Éó„É≠„Ç∏„Çß„ÇØ„Éà„Åß„ÅØ„ÄÅ„É¶„Éº„Ç∂„Éº„ÅÆÂÖ•Âäõ„Å´Âü∫„Å•„ÅÑ„Å¶Èñ¢ÈÄ£„Åô„ÇãÊÉÖÂ†±„ÇíÊ§úÁ¥¢„Åó„ÄÅ„Åù„ÅÆÊÉÖÂ†±„Çí„ÇÇ„Å®„Å´LLM„ÅåÁ≤æÁ∑ª„Å™ÂøúÁ≠î„ÇíÁîüÊàê„Åô„Çã„Ç∑„Çπ„ÉÜ„É†„ÇíÊßãÁØâ„Åô„Çã„ÄÇ

## ‰∏ª„Å™Ê©üËÉΩ

- **Â§ñÈÉ®„Éá„Éº„Çø„ÇΩ„Éº„Çπ„Å®„ÅÆÈÄ£Êê∫:** Mongo DB„ÄÅChroma DBÁ≠â„ÅÆ„Éô„ÇØ„Éà„É´DB„ÇÑOpenAI„ÄÅAnthropic„Å™„Å©„ÅÆLLM API„Å®ÈÄ£Êê∫„Åó„ÄÅÂ§ñÈÉ®„ÅÆÁü•Ë≠ò„ÇíÊ¥ªÁî®„Åó„ÅüÈ´òÂ∫¶„Å™ÊÉÖÂ†±Ê§úÁ¥¢Ê©üËÉΩ„ÇíÂÆüË£Ö„Åô„Çã„ÄÇ

- **ÊÉÖÂ†±Ê§úÁ¥¢„Å®ÁîüÊàê„ÅÆÁµÑ„ÅøÂêà„Çè„Åõ:** „É¶„Éº„Ç∂„Éº„Åã„Çâ„ÅÆË≥™Âïè„Å´ÂØæ„Åó„Å¶„ÄÅ„Åæ„ÅöÈñ¢ÈÄ£„Åô„Çã„Éá„Éº„Çø„ÇíÊ§úÁ¥¢„Åó„ÄÅ„Åù„ÅÆ„Éá„Éº„Çø„ÇíÂü∫„Å´LLM„ÅåÂõûÁ≠î„ÇíÁîüÊàê„Åô„Çã„ÄÇ„Åì„Çå„Å´„Çà„Çä„ÄÅÂæìÊù•„ÅÆLLM„Å†„Åë„Åß„ÅØÈõ£„Åó„ÅÑÁâπÂÆö„ÅÆÊÉÖÂ†±„ÇíÊ≠£Á¢∫„Å´Êèê‰æõ„Åô„Çã„Åì„Å®„ÅåÂèØËÉΩ„Å´„Å™„Çã„ÄÇ


RAG„ÅÆÂà©ÁÇπ„Å®„Åó„Å¶„ÅØ„ÄÅÊ¨°„ÅÆÁÇπ„ÅåÊåô„Åí„Çâ„Çå„Çã:

- **ÊÉÖÂ†±„ÅÆÂ§öÊßòÊÄß:** Ë§áÊï∞„ÅÆÊÉÖÂ†±Ê∫ê„Åã„Çâ„Éá„Éº„Çø„ÇíÂèñÂæó„Åô„Çã„Åü„ÇÅ„ÄÅLLMÂçòÁã¨„Åß„ÅÆÂ≠¶Áøí„Éá„Éº„Çø„Å†„Åë„Åß„ÅØÂæó„Çâ„Çå„Å™„ÅÑÂ§öÊßò„Å™Áü•Ë≠ò„ÇíÁµÑ„ÅøËæº„ÇÄ„Åì„Å®„Åå„Åß„Åç„Çã„ÄÇ

- **Á≤æÂ∫¶„ÅÆÂêë‰∏ä:** „É¶„Éº„Ç∂„Éº„ÅÆË≥™Âïè„Å´ÂØæ„Åó„Å¶„ÄÅÈñ¢ÈÄ£„Åô„ÇãÂÆüÈöõ„ÅÆ„Éá„Éº„Çø„ÇíÁî®„ÅÑ„Çã„Åì„Å®„Åß„ÄÅ„Çà„ÇäÈ´òÁ≤æÂ∫¶„Åß„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„Å´Âç≥„Åó„ÅüÂõûÁ≠î„ÇíÁîüÊàê„Åß„Åç„Çã„ÄÇ

‰ªäÂæå„ÅÆ„Ç¢„ÉÉ„Éó„Éá„Éº„Éà„Å´„Åä„ÅÑ„Å¶„ÅØ„ÄÅRAG„ÇíÁî®„ÅÑ„Åü„Çà„ÇäÈ´òÂ∫¶„Å™Ê©üËÉΩ„ÅÆËøΩÂä†„ÇÑ„ÄÅ„Åï„Çâ„Å´Â§öÊßò„Å™„Éá„Éº„Çø„ÇΩ„Éº„Çπ„Å®„ÅÆÁµ±Âêà„Çí‰∫àÂÆö„Åó„Å¶„ÅÑ„Çã„ÄÇ

## ÂâçÊèêÊù°‰ª∂

- VS Code
- Docker

„Ç§„É≥„Çπ„Éà„Éº„É´„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑÂ†¥Âêà„ÅØ[Installation Guide](./docs/INSTALL.md)„ÇíÂèÇÁÖß„ÄÇ

### „Ç§„É≥„Çπ„Éà„Éº„É´

1. „É™„Éù„Ç∏„Éà„É™„ÅÆ„ÇØ„É≠„Éº„É≥
    ```
    git clone git@github.com:m-higuchi/retrieval-agents.git
    cd retrieval-agents
    ```


> [!CAUTION]
> „É™„Éù„Ç∏„Éà„É™„ÅØÂ∞ÜÊù•ÁöÑ„Å´„ÅØGitHubÁµÑÁπî„Ç¢„Ç´„Ç¶„É≥„Éà„ÅÆ„É™„Éù„Ç∏„Éà„É™„Å´ÁßªË°å‰∫àÂÆö„ÄÇÁßªË°åÂæå„ÅØ„É™„Éù„Ç∏„Éà„É™URL„ÅåÂ§âÊõ¥„Åï„Çå„Çã„Åü„ÇÅË¶ÅÊ≥®ÊÑè„ÄÇ

> [!IMPORTANT]
> „Åì„ÅÆ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅØDev ContainerÁí∞Â¢É„Çí‰ΩøÁî®„Åô„Çã„Åì„Å®„ÇíÊé®Â•®„Åô„Çã„ÄÇÁâπÂà•„Å™ÁêÜÁî±„Åå„Å™„ÅÑÈôê„Çä„ÄÅ‰ª•Èôç„ÅÆ‰ΩúÊ•≠„ÅØ[Dev Container Setup](./docs/DEV_CONTAINER_SETUP.md)„Å´Âæì„Å£„Å¶Dev Container„ÇíÈñã„Åç„ÄÅ„Ç≥„É≥„ÉÜ„ÉäÂÜÖ„Åß‰ΩúÊ•≠„ÇíË°å„ÅÜ„ÄÇ


2. `.env`„Éï„Ç°„Ç§„É´„Çí‰ΩúÊàê„Åô„Çã„ÄÇ

    ```bash
    cp .env.example .env
    ```

3. ‰ΩøÁî®„Åô„ÇãAPI„Éó„É≠„Éê„Ç§„ÉÄ„Éº„ÅÆAPI„Ç≠„ÉºÁ≠â„ÇíË®≠ÂÆö„Åó`.env` „Éï„Ç°„Ç§„É´„Å´‰øùÂ≠ò„Åô„Çã„ÄÇ
    ```
    OPENAI_API_KEY=sk-...
    ```

> [!CAUTION]
> ÁèæÊôÇÁÇπ„Åß„ÅØOpen AI, Anthropic, Nomic AI, Chroma DB„ÅÆ„ÅøÂãï‰ΩúÁ¢∫Ë™çÊ∏à„Åø„ÄÇ

4. „Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„Åä„Çà„Å≥‰æùÂ≠òÈñ¢‰øÇ„Éë„ÉÉ„Ç±„Éº„Ç∏„Çí„Ç§„É≥„Çπ„Éà„Éº„É´„Åô„Çã„ÄÇDev Container„Çí‰ΩøÁî®„Åó„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÄÅ„Åì„ÅÆÊâãÈ†Ü„ÅØ„Ç≥„É≥„ÉÜ„ÉäËµ∑ÂãïÊôÇ„Å´Ëá™ÂãïÂÆüË°å„Åï„Çå„Çã„Åü„ÇÅ‰∏çË¶Å„ÄÇ

    ```
    poetry install --with dev --all-extras
    ```

### „Éá„Éê„ÉÉ„Ç∞„Å®ÂÆüË°å

`langgraph dev --allow-blocking`„Ç≥„Éû„É≥„Éâ„ÇíÂÆüË°å„Åô„Çã„Å®ÈñãÁô∫„É¢„Éº„Éâ„ÅßLangGraph API„Çµ„Éº„Éê„Éº„Åå„É≠„Éº„Ç´„É´„Éõ„Çπ„Éà„ÅßËµ∑Âãï„Åô„Çã„ÄÇ

```
langgraph dev --allow-blocking
```

> [!NOTE]
> ÁèæÁä∂„Åß„ÅØ`--allow-blocking`„Ç™„Éó„Ç∑„Éß„É≥„ÅåÁÑ°„ÅÑ„Å®„Ç®„É©„Éº„ÅåÂá∫„Å¶„Åó„Åæ„ÅÜ„ÄÇÂêåÊúüÁöÑ„Å™I/OÊìç‰Ωú„ÅåÂéüÂõ†„Åß„ÅÇ„Çã„Å®ÊÄù„Çè„Çå„Çã„ÅåËß£Ê∂à„Åß„Åç„Å¶„ÅÑ„Å™„ÅÑ„ÄÇ

Ê≠£Â∏∏„Å´Ëµ∑Âãï„Åô„Çã„Å®‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å´Âá∫Âäõ„Åï„Çå„Çã„ÄÇ

```
INFO:langgraph_api.cli:

        Welcome to

‚ï¶  ‚îå‚îÄ‚îê‚îå‚îê‚îå‚îå‚îÄ‚îê‚ïî‚ïê‚ïó‚î¨‚îÄ‚îê‚îå‚îÄ‚îê‚îå‚îÄ‚îê‚î¨ ‚î¨
‚ïë  ‚îú‚îÄ‚î§‚îÇ‚îÇ‚îÇ‚îÇ ‚î¨‚ïë ‚ï¶‚îú‚î¨‚îò‚îú‚îÄ‚î§‚îú‚îÄ‚îò‚îú‚îÄ‚î§
‚ï©‚ïê‚ïù‚î¥ ‚î¥‚îò‚îî‚îò‚îî‚îÄ‚îò‚ïö‚ïê‚ïù‚î¥‚îî‚îÄ‚î¥ ‚î¥‚î¥  ‚î¥ ‚î¥

- üöÄ API: http://127.0.0.1:2024
- üé® Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024
- üìö API Docs: http://127.0.0.1:2024/docs

This in-memory server is designed for development and testing.
For production use, please use LangGraph Cloud.
```

‰ª•‰∏ã„ÅÆ„ÅÑ„Åö„Çå„Åã„ÅÆÊñπÊ≥ï„ÅßÂÆüË°åÂèØËÉΩ„ÄÇ

- **Web UI:** [Studio UI](http://127.0.0.1:2024)„Å´„Ç¢„ÇØ„Çª„Çπ„Åó„Å¶„ÄÅ„Ç§„É≥„Çø„É©„ÇØ„ÉÜ„Ç£„Éñ„Å´Êìç‰Ωú„ÇíË°å„ÅÜ„ÄÇ
- **API:** [API„Éô„Éº„ÇπURL](http://127.0.0.1:2024)„Å´„Ç¢„ÇØ„Çª„Çπ„Åó„Å¶Áõ¥Êé•API„ÇíÂà©Áî®„Åô„Çã„ÄÇAPI„ÅÆ‰ªïÊßò„ÅØ‰ª•‰∏ã„ÅÆAPD Docs„ÇíÂèÇÁÖß„ÄÇ
- **API Docs:** [API„É™„Éï„Ç°„É¨„É≥„Çπ](http://127.0.0.1:2024/docs)„ÄÇAPI„Ç®„É≥„Éâ„Éù„Ç§„É≥„Éà„ÇíÁõ¥Êé•Ë©¶„Åô„Åì„Å®„Åå„Åß„Åç„ÇãÊ©üËÉΩ„ÇÇÊèê‰æõ„Åó„Å¶„ÅÑ„Çã„ÄÇ

> [!WARNING]
> ‰ª•Èôç„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥„ÅØÂæåÊó•ËøΩÂä†‰∫àÂÆö„ÄÇ

### Setup Retriever

The defaults values for `retriever_provider` are shown below:

```yaml
retriever_provider: elastic
```

Follow the instructions below to get set up, or pick one of the additional options.

#### Elasticsearch

Elasticsearch (as provided by Elastic) is an open source distributed search and analytics engine, scalable data store and vector database optimized for speed and relevance on production-scale workloads.

##### Setup Elasticsearch
Elasticsearch can be configured as the knowledge base provider for a retrieval agent by being deployed on Elastic Cloud (either as a hosted deployment or serverless project) or on your local environment.

**Elasticsearch Serverless**

1. Signup for a free 14 day trial with [Elasticsearch Serverless](https://cloud.elastic.co/registration?onboarding_token=search&cta=cloud-registration&tech=trial&plcmt=article%20content&pg=langchain).
2. Get the Elasticsearch URL, found on home under "Copy your connection details".
3. Create an API key found on home under "API Key".
4. Copy the URL and API key to your `.env` file created above:

```
ELASTICSEARCH_URL=<ES_URL>
ELASTICSEARCH_API_KEY=<API_KEY>
```

**Elastic Cloud**

1. Signup for a free 14 day trial with [Elastic Cloud](https://cloud.elastic.co/registration?onboarding_token=search&cta=cloud-registration&tech=trial&plcmt=article%20content&pg=langchain).
2. Get the Elasticsearch URL, found under Applications of your deployment.
3. Create an API key. See the [official elastic documentation](https://www.elastic.co/search-labs/tutorials/install-elasticsearch/elastic-cloud#creating-an-api-key) for more information.
4. Copy the URL and API key to your `.env` file created above:

```
ELASTICSEARCH_URL=<ES_URL>
ELASTICSEARCH_API_KEY=<API_KEY>
```
**Local Elasticsearch (Docker)**

```
docker run -p 127.0.0.1:9200:9200 -d --name elasticsearch --network elastic-net   -e ELASTIC_PASSWORD=changeme   -e "discovery.type=single-node"   -e "xpack.security.http.ssl.enabled=false"   -e "xpack.license.self_generated.type=trial"   docker.elastic.co/elasticsearch/elasticsearch:8.15.1
```

See the [official Elastic documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/run-elasticsearch-locally.html) for more information on running it locally.

Then populate the following in your `.env` file:

```
# As both Elasticsearch and LangGraph Studio runs in Docker, we need to use host.docker.internal to access.

ELASTICSEARCH_URL=http://host.docker.internal:9200
ELASTICSEARCH_USER=elastic
ELASTICSEARCH_PASSWORD=changeme
```
#### MongoDB Atlas

MongoDB Atlas is a fully-managed cloud database that includes vector search capabilities for AI-powered applications.

1. Create a free Atlas cluster:
- Go to the [MongoDB Atlas website](https://www.mongodb.com/cloud/atlas/register) and sign up for a free account.
- After logging in, create a free cluster by following the on-screen instructions.

2. Create a vector search index
- Follow the instructions at [the Mongo docs](https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-type/)
- By default, we use the collection `langgraph_retrieval_agent.default` - create the index there
- Add an indexed filter for path `user_id`
- **IMPORTANT**: select Atlas Vector Search NOT Atlas Search when creating the index
Your final JSON editor configuration should look something like the following:

```json
{
  "fields": [
    {
      "numDimensions": 1536,
      "path": "embedding",
      "similarity": "cosine",
      "type": "vector"
    },
    {
      "path": "user_id",
      "type": "filter"
    }
  ]
}
```

The exact numDimensions may differ if you select a different embedding model.

2. Set up your environment:
- In the Atlas dashboard, click on "Connect" for your cluster.
- Choose "Connect your application" and copy the provided connection string.
- Create a `.env` file in your project root if you haven't already.
- Add your MongoDB Atlas connection string to the `.env` file:

```
MONGODB_URI="mongodb+srv://username:password@your-cluster-url.mongodb.net/?retryWrites=true&w=majority&appName=your-cluster-name"
```

Replace `username`, `password`, `your-cluster-url`, and `your-cluster-name` with your actual credentials and cluster information.
#### Pinecone Serverless

Pinecone is a managed, cloud-native vector database that provides long-term memory for high-performance AI applications.

1. Sign up for a Pinecone account at [https://login.pinecone.io/login](https://login.pinecone.io/login) if you haven't already.

2. After logging in, generate an API key from the Pinecone console.

3. Create a serverless index:
   - Choose a name for your index (e.g., "example-index")
   - Set the dimension based on your embedding model (e.g., 1536 for OpenAI embeddings)
   - Select "cosine" as the metric
   - Choose "Serverless" as the index type
   - Select your preferred cloud provider and region (e.g., AWS us-east-1)

4. Once you have created your index and obtained your API key, add them to your `.env` file:

```
PINECONE_API_KEY=your-api-key
PINECONE_INDEX_NAME=your-index-name
```


### Setup Model

The defaults values for `response_model`, `query_model` are shown below:

```yaml
response_model: anthropic/claude-3-5-sonnet-20240620
query_model: anthropic/claude-3-haiku-20240307
```

Follow the instructions below to get set up, or pick one of the additional options.

#### Anthropic

To use Anthropic's chat models:

1. Sign up for an [Anthropic API key](https://console.anthropic.com/) if you haven't already.
2. Once you have your API key, add it to your `.env` file:

```
ANTHROPIC_API_KEY=your-api-key
```
#### OpenAI

To use OpenAI's chat models:

1. Sign up for an [OpenAI API key](https://platform.openai.com/signup).
2. Once you have your API key, add it to your `.env` file:
```
OPENAI_API_KEY=your-api-key
```



### Setup Embedding Model

The defaults values for `embedding_model` are shown below:

```yaml
embedding_model: openai/text-embedding-3-small
```

Follow the instructions below to get set up, or pick one of the additional options.

#### OpenAI

To use OpenAI's embeddings:

1. Sign up for an [OpenAI API key](https://platform.openai.com/signup).
2. Once you have your API key, add it to your `.env` file:
```
OPENAI_API_KEY=your-api-key
```

#### Cohere

To use Cohere's embeddings:

1. Sign up for a [Cohere API key](https://dashboard.cohere.com/welcome/register).
2. Once you have your API key, add it to your `.env` file:

```bash
COHERE_API_KEY=your-api-key
```





<!--
End setup instructions
-->

## Using

```bash
```
Once you've set up your retriever saved your model secrets, it's time to try it out! First, let's add some information to the index. Open studio, select the "indexer" graph from the dropdown in the top-left, provide an example user ID in the configuration at the bottom, and then add some content to chat over.

```json
[{ "page_content": "My cat knows python." }]
```

When you upload content, it will be indexed under the configured user ID. You know it's complete when the indexer "delete"'s the content from its graph memory (since it's been persisted in your configured storage provider).

Next, open the "retrieval_graph" using the dropdown in the top-left. Ask it about your cat to confirm it can fetch the required information! If you change the `user_id` at any time, notice how it no longer has access to your information. The graph is doing simple filtering of content so you only access the information under the provided ID.

## Development

While iterating on your graph, you can edit past state and rerun your app from past states to debug specific nodes. Local changes will be automatically applied via hot reload. Try adding an interrupt before the agent calls tools, updating the default system message in `src/retrieval_agents/agents/utils.py` to take on a persona, or adding additional nodes and edges!

Follow up requests will be appended to the same thread. You can create an entirely new thread, clearing previous history, using the `+` button in the top right.

You can find the latest (under construction) docs on [LangGraph](https://github.com/langchain-ai/langgraph) here, including examples and other references. Using those guides can help you pick the right patterns to adapt here for your use case.

LangGraph Studio also integrates with [LangSmith](https://smith.langchain.com/) for more in-depth tracing and collaboration with teammates.