"""Agent for adaptive RAG."""

import logging
from typing import Annotated, Dict, Literal, Sequence, cast

from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.types import Command
from pydantic import BaseModel, Field

from retrieval_agents import prompts
from retrieval_agents.configurations import ConfigurationBase
from retrieval_agents.modules.states import BasicRAGInputState
from retrieval_agents.modules.utils import load_chat_model, reduce_docs

logger = logging.getLogger("adaptive_rag_graph")


### Configuration ###
class ContextualAnswerGeneratorConfiguration(ConfigurationBase):
    """The configuration for the adaptive rag agent."""

    answer_grader_model: Annotated[str, {"__metadata__": {"kind": "llm"}}] = Field(
        default="openai/gpt-4o",
        description="The language model used for grading the answer generated by the LLM whether it based on the facts",
    )

    answer_grader_system_prompt: str = Field(
        default=prompts.ANSWER_GRADER_SYSTEM_PROMPT,
        description="The system prompt for grading the generated answer.",
    )

    answer_grader_human_prompt: str = Field(
        default=prompts.ANSWER_GRADER_HUMAN_PROMPT,
        description="The human prompt for grading the generated answer.",
    )

    generate_human_prompt: str = Field(
        default=prompts.GENERATE_HUMAN_PROMPT,
        description="The prompt used for answering the question based on the retrieved context.",
    )

    hallucination_grader_model: Annotated[
        str, {"__template__metadata__"} : {"kind": "llm"}
    ] = Field(
        default="openai/gpt-4o",
        description="The language model used for grading hallucination.",
    )

    hallucination_grader_system_prompt: str = Field(
        default=prompts.HALLUCINATION_GRADER_SYSTEM_PROMPT,
        description="The prompt used for grading the answer generated by the LLM whether it based on the facts.",
    )

    hallucination_grader_human_prompt: str = Field(
        default=prompts.HALLUCINATION_GRADER_HUMAN_PROMPT,
        description="The human prompt used for grading the answer gerated by the LLM whether it based on the facts",
    )

    grade_documents_model: Annotated[
        str, {"__template_metadata__": {"kind": "llm"}}
    ] = Field(
        default="openai/gpt-4o",
        description="The language model used for grading the documents.",
    )

    grade_documents_system_prompt: str = Field(
        default=prompts.GRADE_DOCUMENTS_SYSTEM_PROMPT,
        description="The system prompt used for grading the documents.",
    )

    grade_documents_human_prompt: str = Field(
        default=prompts.GRADE_DOCUMENTS_HUMAN_PROMPT,
        description="The human prompt used for grading the documents.",
    )

    generate_model: Annotated[str, {"__template_metadata__": {"kind": "llm"}}] = Field(
        default="openai/gpt-4o", description="The language model used for generating."
    )


### Schemas ###
class GradeHallucinations(BaseModel):
    """Binary score for hallucination present in generation answer."""

    binary_score: str = Field(
        description="Answer is grounded in the facts, 'yes' or 'no'"
    )


class GradeAnswer(BaseModel):
    """Binary score to assess answer addresses question."""

    binary_score: str = Field(
        description="Answer addresses the question, 'yes' or 'no'"
    )


class GradeDocuments(BaseModel):
    """Binary score for relevance check on retrieved documents."""

    binary_score: str = Field(
        description="Documents are relevant to the question, 'yes' or 'no'"
    )


### States ###
class ContextualAnswerGeneratorInputState(BasicRAGInputState):
    """State for the contextual answer generator input."""

    documents: Annotated[Sequence[Document], reduce_docs]


class ContextualAnswerGeneratorState(ContextualAnswerGeneratorInputState):
    """State for the contextual answer generator."""

    generation: str = Field(default="")
    finish_reason: str = Field(default="")


### Nodes ###
async def grade_context(
    state: ContextualAnswerGeneratorInputState, *, config: RunnableConfig
) -> Command[Literal["generate", "__end__"]]:
    """Determine whether the retrieved documents are relevant to the question.

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): Updates documents key with only filtered relevant documents
    """
    logger.info(f"grade_documents_call: {state}")
    configuration = ContextualAnswerGeneratorConfiguration.from_runnable_config(config)
    question = state.question
    documents = state.documents

    llm = load_chat_model(configuration.grade_documents_model)
    structured_llm_grader = llm.with_structured_output(
        GradeDocuments.model_json_schema()
    )

    # Prompt
    system = configuration.grade_documents_system_prompt
    grade_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system),
            ("human", configuration.grade_documents_human_prompt),
        ]
    )

    retrieval_grader = grade_prompt | structured_llm_grader

    # Score each doc
    filtered_docs = []
    for d in documents:
        score = cast(
            Dict[str, str],
            await retrieval_grader.ainvoke(
                {"question": question, "document": d.page_content}
            ),
        )
        grade = score["binary_score"]
        if grade == "yes":
            logger.info("GRADE: DOCUMENT RELEVANT")
            filtered_docs.append(d)
        else:
            logger.info("GRADE: DOCUMENT NOT RELEVANT")
            continue
    if len(filtered_docs) == 0:
        return Command(
            goto="__end__",
            update={"question": question, "finish_reason": "no_relevant_documents"},
        )
    else:
        return Command(
            goto="generate", update={"question": question, "documents": filtered_docs}
        )


async def generate(
    state: ContextualAnswerGeneratorInputState, *, config: RunnableConfig
) -> dict[str, str | Sequence[Document]]:
    """Generate answer.

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, generation, that contains LLM generation
    """
    configuration = ContextualAnswerGeneratorConfiguration.from_runnable_config(config)

    question = state.question
    documents = state.documents
    prompt = ChatPromptTemplate.from_messages(
        [("human", configuration.generate_human_prompt)]
    )
    llm = load_chat_model(configuration.generate_model)
    rag_chain = prompt | llm | StrOutputParser()
    # RAG generation
    generation = await rag_chain.ainvoke({"context": documents, "question": question})

    return {"documents": documents, "question": question, "generation": generation}


async def grade_generation(
    state: ContextualAnswerGeneratorState, *, config: RunnableConfig
) -> Command[Literal["generate", "__end__"]]:
    """Determine whether the generation is grounded in the document and answers question.

    Args:
        state (dict): The current graph state

    Returns:
        str: Decision for next node to call
    """
    configuration = ContextualAnswerGeneratorConfiguration.from_runnable_config(config)
    grade_hallucination = (
        await _grade_generation_v_documents_and_question_hallucination(
            state=state, configuration=configuration
        )
    )

    # Check hallucination
    if grade_hallucination:
        grade_answer = await _grade_generation_v_docuemnts_and_question_answer(
            state=state, configuration=configuration
        )
        if not grade_answer:
            logger.info("DECISION: GENERATION DOES NOT ADDRESS QUESTION")
            return Command(
                goto="__end__",
                update={
                    "question": state.question,
                    "documents": state.documents,
                    "generation": state.generation,
                    "finish_reason": "not_useful",
                },
            )

        else:
            logger.info("DECISION: GENERATION ADDRESSES QUESTION")
            return Command(
                goto="__end__",
                update={
                    "question": state.question,
                    "documents": state.documents,
                    "generation": state.generation,
                    "finish_reason": "complete",
                },
            )
    else:
        logger.info("DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY")
        return Command(
            goto="generate",
            update={"question": state.question, "documents": state.documents},
        )


async def _grade_generation_v_documents_and_question_hallucination(
    state: ContextualAnswerGeneratorState,
    configuration: ContextualAnswerGeneratorConfiguration,
) -> bool:
    documents = state.documents
    generation = state.generation

    llm = load_chat_model(configuration.hallucination_grader_model)
    configuration.hallucination_grader_human_prompt
    structured_llm_grader = llm.with_structured_output(
        GradeHallucinations.model_json_schema(), include_raw=True
    )
    hallucination_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", configuration.hallucination_grader_system_prompt),
            ("human", configuration.hallucination_grader_human_prompt),
        ]
    )
    hallucination_grader = hallucination_prompt | structured_llm_grader
    response = cast(
        Dict[str, Dict[str, str]],
        await hallucination_grader.ainvoke(
            {"documents": documents, "generation": generation}
        ),
    )
    # _ = response["raw"]
    score = response["parsed"]
    # _ = response["parsing_error"]

    grade = score["binary_score"]
    if grade == "yes":
        return True
    else:
        return False


async def _grade_generation_v_docuemnts_and_question_answer(
    state: ContextualAnswerGeneratorState,
    configuration: ContextualAnswerGeneratorConfiguration,
) -> bool:
    question = state.question
    generation = state.generation
    # Check question-answering
    llm = load_chat_model(configuration.answer_grader_model)
    structured_llm_grader = llm.with_structured_output(GradeAnswer.model_json_schema())
    answer_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", configuration.answer_grader_system_prompt),
            ("human", configuration.answer_grader_human_prompt),
        ]
    )

    answer_grader = answer_prompt | structured_llm_grader
    score = cast(
        Dict[str, str],
        await answer_grader.ainvoke({"question": question, "generation": generation}),
    )
    grade = score["binary_score"]
    if grade == "yes":
        return True
    else:
        return False


### Edges ###

### Build Graph ###
graph_name = "ContextualAnswerGeneratorGraph"
logger.info(f"Building {graph_name}")
builder = StateGraph(
    ContextualAnswerGeneratorState,
    input=ContextualAnswerGeneratorInputState,
    config_schema=ContextualAnswerGeneratorConfiguration,
)

builder.add_node(grade_context)
builder.add_node(generate)
builder.add_node(grade_generation)

builder.set_entry_point("grade_context")
builder.add_edge("generate", "grade_generation")


graph: CompiledStateGraph = builder.compile(
    interrupt_before=[],  # if you want to update the state before calling the tools
    interrupt_after=[],
)
graph.name = graph_name
